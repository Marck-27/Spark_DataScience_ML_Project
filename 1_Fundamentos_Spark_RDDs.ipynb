{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentos de Apache Spark: RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este notebook trabajaremos con los RDDs que forma parte del Spark Core.La implementación de Spark Core es un **RDD (Resilient Distributed Dataset)** que es una colección de datos distribuidos en diferentes nodos del clúster que se procesan en paralelo.\n",
    "\n",
    "Utilizaremos la API de PySpark, pero los conceptos aplican por igual a todas las APIs (Scala, R, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inicialización de Spark en Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init()\n",
    "#import pandas as pd\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear el SparkSession y el SparkContext\n",
    "\n",
    "El \"SparkSession\" es el punto de entrada a Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('PySpark_training')\\\n",
    "        .getOrCreate()\n",
    "\n",
    "# SparkSession <-- Punto de entrada a Apache Spark, crea internamente el \"SparkConfig\" y el \"SparkContext\"\n",
    "# .master(\"local[*]\") <-- si estamos ejecutando este Notebook en un cluster, debemos indicar que estamos en el nodo maestro \".master()\" \n",
    "# \"local[*]\" <-- indica usar todos los nucleos de nuestra computadora local, si queremos usar 2 nucleos deberiamos poner \"local[2]\"\n",
    "#                el número de nucleos que se especifique será el tamaño en que se partirán los RDDs\n",
    "# appName() <-- Sirve para especificar el nombre de la aplicación Spark\n",
    "# .getOrCreate() <-- Crea un SparkSession ya existente o crea uno nuevo con las configuraciones indicadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detenemos la \"SparkSession\":\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.76:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark_training</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f2207f15670>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos una SparkSession con las configuraciones por default:\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.76:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark_training</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySpark_training>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Iniciamos el SparkContext\n",
    "sc = spark.sparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crear un RDD de una lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n"
     ]
    }
   ],
   "source": [
    "# Creamos una lista de números:\n",
    "num = list( range(20) )\n",
    "print( num )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos el RDD usando \"parallelize\"\n",
    "# esto permite particinar los datos entre todos los nodos del clúster\n",
    "num_rdd = sc.parallelize(num)\n",
    "num_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación aplicamos la acción \"collect()\" para coleccionar la data en el nodo maestro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coleccionamos los datos paralelizados en el nodo maestro:\n",
    "num_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformaciones\n",
    "* Como sabemos, las Transformaciones son de naturaleza perezosa y no se ejecutarán hasta que se ejecute una Acción sobre ellas.\n",
    "* Intentemos comprender las distintas transformaciones disponibles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### map\n",
    "* Esto mapeará su entrada a alguna salida basada en la función especificada en la función "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34, 36, 38]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "double_rdd = num_rdd.map(lambda x : x * 2)\n",
    "double_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### filtro\n",
    "* Para filtrar los datos en función de una determinada condición. Intentemos encontrar los números pares de num_rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8, 10, 12, 14, 16, 18]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "even_rdd = num_rdd.filter(lambda x : x % 2 == 0)\n",
    "even_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flatMap\n",
    "* Esta función es muy similar a map, pero puede devolver múltiples elementos para cada entrada en el RDD dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 8, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n"
     ]
    }
   ],
   "source": [
    "flat_rdd = num_rdd.flatMap(lambda x : range(0,x) )\n",
    "\n",
    "# colecciona los siguientes elementos en un sólo RDD plano \"flat\":\n",
    "# 0 -> [0]\n",
    "# 1 -> [0]\n",
    "# 2 -> [0,1]\n",
    "# 3 -> [0,1,2]\n",
    "# 4 -> [0,1,2,3]\n",
    "# 5 -> [0,1,2,3,4], ...\n",
    "\n",
    "print( flat_rdd.collect() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distinct\n",
    "* Esto devolverá elementos distintos de un RDD, es decir los valores representativos de un RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos un RDD:\n",
    "rdd1 = sc.parallelize( [10, 11, 2, 10, 11, 12, 11,2,2] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 2, 11, 12]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_rdd = rdd1.distinct()\n",
    "dist_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reduceByKey\n",
    "* Esta función reduce los pares de valores clave en función de las claves y una función determinada dentro de reduceByKey\n",
    "* Reduce la cantidad de elementos que hay en una lista de tuplas, [(’llave_1’,valor_1) , (’llave_2’,valor_2),...,(’llave_n’,valor_n) ], de acuerdo a una función f que se aplica a los valores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 5), ('b', 7), ('c', 2), ('a', 3), ('b', 1), ('c', 4), ('a', 2)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos una lista de tuplas (\"clave\",valor)\n",
    "pairs = [ (\"a\", 5), (\"b\", 7), (\"c\", 2), (\"a\", 3), (\"b\", 1), (\"c\", 4) , ('a', 2),]\n",
    "\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 5), ('b', 7), ('c', 2), ('a', 3), ('b', 1), ('c', 4), ('a', 2)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pasamos la tupla de lista a un RDD:\n",
    "pair_rdd = sc.parallelize(pairs)\n",
    "\n",
    "# Coleccionamos valores:\n",
    "pair_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 10), ('b', 8), ('c', 6)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicamos reduceByKey de acuerdo a la función \"f\":\n",
    "output = pair_rdd.reduceByKey(lambda val_1, val_2 : f(val_1, val_2))\n",
    "\n",
    "result = output.collect()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupByKey\n",
    "* Esta función es otra función ByKey que puede operar en un par (clave, valor) RDD pero esto solo agrupará los valores basados en las claves. En otras palabras, esto solo realizará el primer paso de reduceByKey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', <pyspark.resultiterable.ResultIterable at 0x7f2220143250>),\n",
       " ('b', <pyspark.resultiterable.ResultIterable at 0x7f2207f262b0>),\n",
       " ('c', <pyspark.resultiterable.ResultIterable at 0x7f2207f262e0>)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_out = pair_rdd.groupByKey()\n",
    "grp_out.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sortByKey\n",
    "* Esta función realizará la clasificación en un par (clave, valor) RDD basado en las claves. De forma predeterminada, la clasificación se realizará en orden ascendente.\n",
    "* Ordena ascendentemente los elementos que hay en una lista de tuplas, [(’llave_1’,valor_1) , (’llave_2’,valor_2),...,(’llave_n’,valor_n) ], de acuerdo a los valores de cada tupla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 5), ('b', 3), ('c', 2), ('c', 5), ('d', 7)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = [ (\"a\", 5), (\"d\", 7), (\"c\", 2), (\"b\", 3), (\"c\", 5) ]\n",
    "raw_rdd = sc.parallelize(pairs)\n",
    "\n",
    "sortkey_rdd = raw_rdd.sortByKey(ascending=True)\n",
    "result = sortkey_rdd.collect()\n",
    "result\n",
    "\n",
    "# Para clasificar en orden descendente, pase  “ascending=False”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordenar por\n",
    "* sortBy es una función más generalizada para ordenar.\n",
    "*  Ordena ascendentemente los elementos que hay en una lista de tuplas con más de 2 elementos. El ordenamiento lo hace con respecto a los valores de la “n-ésima” componente de cada tupla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('b', 3, 9), ('a', 5, 10), ('c', 2, 11), ('d', 7, 12)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creamos una lista de tuplas:\n",
    "tuplas_general = [ (\"a\", 5, 10), (\"d\", 7, 12), (\"c\", 2, 11), (\"b\", 3, 9) ]\n",
    "\n",
    "# Creamos el RDD:\n",
    "raw_rdd = sc.parallelize(tuplas_general)\n",
    "\n",
    "# Ordenamos de acuerdo al tercer elemento de las tuplas:\n",
    "sort_out = raw_rdd.sortBy(lambda x : x[2])\n",
    "result = sort_out.collect()\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByKey().items()\n",
    "* Permite extraer el número de véces en que aparece una tupla con un mismo 'key'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('a', 2), ('b', 3), ('c', 1)])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [ ('a',6) , ('b',3), ('c',1), ('b',2), ('a',1), ('b',1) ]\n",
    "\n",
    "data_rdd = sc.parallelize(data)\n",
    "\n",
    "data_rdd.countByKey().items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### countByValue().items()\n",
    "* Permite extraer el número de véces en que aparece cada valor en un RDD, sirve para generar distribuciones de frecuencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([('Python', 5), ('Scala', 3), ('R', 2)])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ['Python' , 'Python' , 'Scala' , 'Python', 'R', 'Scala', 'R', 'Scala', 'Python', 'Python']\n",
    "\n",
    "# Creamos el RDD:\n",
    "data_rdd = sc.parallelize(data)\n",
    "\n",
    "data_rdd.countByValue().items() # Regresa el conteo de valores por cada item "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acciones\n",
    "\n",
    "* Las acciones son operaciones en RDD que se ejecutan inmediatamente. Mientras que las transformaciones devuelven otro RDD, las acciones devuelven estructuras de datos nativas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### count\n",
    "* Esto contará el número de elementos en el RDD dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd = sc.parallelize([10,2,3,4,2])\n",
    "num_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### first\n",
    "* Esto devolverá el primer elemento del RDD dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect\n",
    "* Esto devolverá todos los elementos para el RDD dado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 2, 3, 4, 2]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**No debemos utilizar la operación de collect mientras trabajamos con grandes conjuntos de datos**. Porque devolverá todos los datos que se distribuyen entre los diferentes trabajadores dl clúster a un controlador. Todos los datos viajarán a través de la red del trabajador al conductor y también el conductor necesitaría almacenar todos los datos. Esto obstaculizará el rendimiento de su aplicación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take\n",
    "* Esto devolverá los primeros \"n\" elementos del RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 2, 3]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=3\n",
    "num_rdd.take(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apagamos la SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Más transformaciones y acciones se pueden consultar [aquí](https://spark.apache.org/docs/latest/rdd-programming-guide.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
